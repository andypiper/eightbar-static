<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>webcam &#8211; eightbar</title>
	<atom:link href="http://eightbar.co.uk/tag/webcam/feed/" rel="self" type="application/rss+xml" />
	<link>http://eightbar.co.uk</link>
	<description>Raising The Eight Bar</description>
	<lastBuildDate>Tue, 26 Jul 2016 07:41:27 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.5.3</generator>
	<item>
		<title>Smile!</title>
		<link>http://eightbar.co.uk/2012/04/03/smile/</link>
		<pubDate>Tue, 03 Apr 2012 22:29:32 +0000</pubDate>
		<dc:creator><![CDATA[Dale Lane]]></dc:creator>
				<category><![CDATA[Planet]]></category>
		<category><![CDATA[code]]></category>
		<category><![CDATA[eightbar]]></category>
		<category><![CDATA[face]]></category>
		<category><![CDATA[python]]></category>
		<category><![CDATA[webcam]]></category>

		<guid isPermaLink="false">http://dalelane.co.uk/blog/?p=2092</guid>
		<description><![CDATA[The visualisations on this page need Flash and Javascript. Apologies if that means most of this page doesn&#8217;t work for you! This is my mood (as identified from my facial expressions) over time while watching Never Mind the Buzzcocks. The green areas are times where I looked happy. This shows my mood while playing XBox [...] <a href="http://eightbar.co.uk/2012/04/03/smile/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>This is my mood (as identified from my facial expressions) over time while watching <a href="http://www.bbc.co.uk/programmes/b006v0dz">Never Mind the Buzzcocks</a>. </p>
<p><img src="http://i267.photobucket.com/albums/ii311/dale_lane/120403visualisationone.png"/></p>
<p>The green areas are times where I looked happy.</p>
<p>This shows my mood while playing XBox Live. Badly. </p>
<p><img src="http://i267.photobucket.com/albums/ii311/dale_lane/120403visualisationtwo.png"/></p>
<p>The red areas are times where I looked cross. </p>
<p>I smile more while watching comedies than when getting shot in the head. Shocker, eh? </p>
<p>A couple of years ago, I played with the idea of <a href="http://dalelane.co.uk/blog/?p=1176">capturing my TV viewing habits and making some visualisations</a> from them. This is a sort of return to that idea in a way. </p>
<p>A webcam lives on the top of our TV, mainly for skype calls. I was thinking that when watching TV, we&#8217;re often more or less looking at the webcam. What could it capture? </p>
<p>What about keeping track of how much I smile while watching a comedy, as a way of measuring which comedies I find funnier? </p>
<p><img src="http://dalelane.co.uk/blog/post-images/120403-smiling1.jpg"/></p>
<p>This suggests that, overall, I might&#8217;ve found Mock the Week funnier. But, this shows my facial expressions while watching <a href="http://www.bbc.co.uk/programmes/b006t6vf">Mock the Week</a>. </p>
<p><img src="http://i267.photobucket.com/albums/ii311/dale_lane/120403visualisationthree.png"/></p>
<p>It seems that, unlike with Buzzcocks, I really enjoyed the beginning bit, then perhaps got a bit less enthusiastic after a bit. </p>
<p>What about <a href="http://www.thedailyshow.com/">The Daily Show with Jon Stewart</a>?</p>
<p><img src="http://i267.photobucket.com/albums/ii311/dale_lane/120403visualisationsix.png"/></p>
<p>I think the two neutral bits are breaks for adverts. </p>
<p>Or classifying facial expressions by mood and looking for the dominant mood while watching something more serious on TV? </p>
<p>This shows my facial expressions while catching a bit of <a href="http://www.bbc.co.uk/programmes/b006mk25">Newsnight</a>.</p>
<p><img src="http://i267.photobucket.com/albums/ii311/dale_lane/120403visualisationfour.png"/></p>
<p>On the whole, my expression remained reasonably neutral whilst watching the news, but you can see where I visibly reacted to a few of the news items. </p>
<p>Or looking to see how I react to playing different games on the XBox?</p>
<p>This shows my facial expressions while playing Modern Warfare 3 last night.</p>
<p><img src="http://i267.photobucket.com/albums/ii311/dale_lane/120403visualisationfive.png"/></p>
<p>Mostly &#8220;sad&#8221;, as I kept getting shot in the head. With occasional moments where something made me smile or laugh, presumably when something went well. </p>
<p>Compare that with what I looked like while playing Blur (a car racing game).</p>
<p><img src="http://i267.photobucket.com/albums/ii311/dale_lane/120403visualisationtwo.png"/></p>
<p>It seems that I looked a little more aggressive while driving than running around getting shot. For last night, at any rate. </p>
<p><strong>Not just about watching TV</strong></p>
<p>I&#8217;m using face recognition to tell my expressions apart from other people in the room. This means there is also a bunch of stuff I could look into around how my expressions change based on who else is in the room, and their expressions? </p>
<p>For example, looking at how much of the time I spend smiling when I&#8217;m the only one in the room, compared with when one or both of <a href="https://picasaweb.google.com/dale.lane">my kids</a> are in the room. </p>
<p><img src="http://dalelane.co.uk/blog/post-images/120403-smiling2.jpg"/></p>
<p>To be fair, this isn&#8217;t a scientific comparison. There are lots of factors here &#8211; for example, when the girls are in the room, I&#8217;ll probably be doing a different activity (such as playing a game with them or reading a story) to what I would be doing when by myself (typically doing some work on my laptop, or reading). This could be showing how much I smile based on which activity I&#8217;m doing. But I thought it was a cute result, anyway. </p>
<p><strong>Limitations</strong></p>
<p>This isn&#8217;t sophisticated stuff. </p>
<p>The webcam is an old, cheap one that only has a maximum resolution of 640&#215;480, and I&#8217;m sat at the other end of the room to it. I can&#8217;t capture fine facial detail here. </p>
<p>I&#8217;m not doing anything complicated with video feeds. I&#8217;m just sampling by taking photos at regular intervals. You could reasonably argue that the funniest joke in the world isn&#8217;t going to get me to sustain a broad smile for over a minute, so there is a lot being missed here. </p>
<p>And my y-axis is a little suspect. I&#8217;m using the percentage level of confidence that the classifier had in identifying the mood. I&#8217;m doing this on the assumption that the more confident the classifier was, the stronger or more pronounced my facial expression probably was. </p>
<p>Regardless of all of this, I think the idea is kind of interesting. </p>
<p><strong>How does it work?</strong></p>
<p>The <a href="http://dalelane.co.uk/blog/?p=1228">media server under the TV</a> runs Ubuntu, so I had a lot of options. My language-of-choice for quick hacks is Python, so I used <a href="http://www.pygame.org/">pygame</a> to capture stills from the webcam.</p>
<p>For the complicated facial stuff, I&#8217;m using <a href="http://developers.face.com/">web services from face.com</a>. </p>
<p>They have a REST API for uploading a photo to, getting back a blob of JSON with information about faces detected in the photo. This includes a guess at the gender, a description of mood from the facial expression, whether the face is smiling, and even an estimated age (often not complimentary!). </p>
<p>I used a <a href="https://github.com/chris-piekarski/python-face-client">Python client library from github</a> to build the requests, so getting this working took no time at all. </p>
<p>There is a face recognition REST API. You can train the system to recognise certain faces. I didn&#8217;t write any code to do this, as I don&#8217;t need to do it again, so I did this using the <a href="http://developers.face.com/tools/#faces/detect">API sandbox on the face.com website</a>. I gave it a dozen or so photos with my face in, which seemed to be more than enough for the system to be able to tell me apart from someone else in the room. </p>
<p>My monitoring code puts what it measures about me in one log, and what it measures about anyone else in a second &#8220;guest log&#8221;. </p>
<p>This is the result of one evening&#8217;s playing, so I&#8217;ve not really finished with this. I think there is more to do with it, but for what it&#8217;s worth, this is what I&#8217;ve come up with so far. </p>
<p><strong>The script</strong></p>
<pre style="border: thin solid silver; color: black; background-color: #eeeeee; padding: 0.7em; overflow: auto;">####################################################
#  IMPORTS
####################################################

# imports for capturing a frame from the webcam
import pygame.camera
import pygame.image

# import for detecting faces in the photo
import face_client

# import for storing data
from pysqlite2 import dbapi2 as sqlite

# miscellaneous imports
from time import strftime, localtime, sleep
import os
import sys

####################################################
# CONSTANTS
####################################################

DB_FILE_PATH="/home/dale/dev/audiencemonitor/data/log.db"
FACE_COM_APIKEY="MY_API_KEY_HERE"
FACE_COM_APISECRET="MY_API_SECRET_HERE"
DALELANE_FACETAG="dalelane@dale.lane"
POLL_FREQUENCY_SECONDS=3

class AudienceMonitor():

    #
    # prepare the database where we store the results
    #
    def initialiseDB(self):
        self.connection = sqlite.connect(DB_FILE_PATH, detect_types=sqlite.PARSE_DECLTYPES|sqlite.PARSE_COLNAMES)
        cursor = self.connection.cursor()

        cursor.execute('SELECT name FROM sqlite_master WHERE type="table" AND NAME="facelog" ORDER BY name')
        if not cursor.fetchone():
            cursor.execute('CREATE TABLE facelog(ts timestamp unique default current_timestamp, isSmiling boolean, smilingConfidence int, mood text, moodConfidence int)')

        cursor.execute('SELECT name FROM sqlite_master WHERE type="table" AND NAME="guestlog" ORDER BY name')
        if not cursor.fetchone():
            cursor.execute('CREATE TABLE guestlog(ts timestamp unique default current_timestamp, isSmiling boolean, smilingConfidence int, mood text, moodConfidence int, agemin int, ageminConfidence int, agemax int, agemaxConfidence int, ageest int, ageestConfidence int, gender text, genderConfidence int)')

        self.connection.commit()

    #
    # initialise the camera
    #
    def prepareCamera(self):
        # prepare the webcam
        pygame.camera.init()
        self.camera = pygame.camera.Camera(pygame.camera.list_cameras()[0], (900, 675))
        self.camera.start()

    #
    # take a single frame and store in the path provided
    #
    def captureFrame(self, filepath):
        # save the picture
        image = self.camera.get_image()
        pygame.image.save(image, filepath)

    #
    # gets a string representing the current time to the nearest second
    #
    def getTimestampString(self):
        return strftime("%Y%m%d%H%M%S", localtime())

    #
    # get attribute from face detection response
    #
    def getFaceDetectionAttributeValue(self, face, attribute):
        value = None
        if attribute in face['attributes']:
            value = face['attributes'][attribute]['value']
        return value

    #
    # get confidence from face detection response
    #
    def getFaceDetectionAttributeConfidence(self, face, attribute):
        confidence = None
        if attribute in face['attributes']:
            confidence = face['attributes'][attribute]['confidence']
        return confidence

    #
    # detects faces in the photo at the specified path, and returns info
    #
    def faceDetection(self, photopath):
        client = face_client.FaceClient(FACE_COM_APIKEY, FACE_COM_APISECRET)
        response = client.faces_recognize(DALELANE_FACETAG, file_name=photopath)
        faces = response['photos'][0]['tags']
        for face in faces:
            userid = ""
            faceuseridinfo = face['uids']
            if len(faceuseridinfo) &gt; 0:
                userid = faceuseridinfo[0]['uid']
            if userid == DALELANE_FACETAG:
                smiling = self.getFaceDetectionAttributeValue(face, "smiling")
                smilingConfidence = self.getFaceDetectionAttributeConfidence(face, "smiling")
                mood = self.getFaceDetectionAttributeValue(face, "mood")
                moodConfidence = self.getFaceDetectionAttributeConfidence(face, "mood")
                self.storeResults(smiling, smilingConfidence, mood, moodConfidence)
            else:
                smiling = self.getFaceDetectionAttributeValue(face, "smiling")
                smilingConfidence = self.getFaceDetectionAttributeConfidence(face, "smiling")
                mood = self.getFaceDetectionAttributeValue(face, "mood")
                moodConfidence = self.getFaceDetectionAttributeConfidence(face, "mood")
                agemin = self.getFaceDetectionAttributeValue(face, "age_min")
                ageminConfidence = self.getFaceDetectionAttributeConfidence(face, "age_min")
                agemax = self.getFaceDetectionAttributeValue(face, "age_max")
                agemaxConfidence = self.getFaceDetectionAttributeConfidence(face, "age_max")
                ageest = self.getFaceDetectionAttributeValue(face, "age_est")
                ageestConfidence = self.getFaceDetectionAttributeConfidence(face, "age_est")
                gender = self.getFaceDetectionAttributeValue(face, "gender")
                genderConfidence = self.getFaceDetectionAttributeConfidence(face, "gender")
                # if the face wasnt recognisable, it might've been me after all, so ignore
                if "tid" in face and face['recognizable'] == True:
                    self.storeGuestResults(smiling, smilingConfidence, mood, moodConfidence, agemin, ageminConfidence, agemax, agemaxConfidence, ageest, ageestConfidence, gender, genderConfidence)
                    print face['tid']

    #
    # stores face results in the DB
    #
    def storeGuestResults(self, smiling, smilingConfidence, mood, moodConfidence, agemin, ageminConfidence, agemax, agemaxConfidence, ageest, ageestConfidence, gender, genderConfidence):
        cursor = self.connection.cursor()
        cursor.execute('INSERT INTO guestlog(isSmiling, smilingConfidence, mood, moodConfidence, agemin, ageminConfidence, agemax, agemaxConfidence, ageest, ageestConfidence, gender, genderConfidence) values(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)',
                        (smiling, smilingConfidence, mood, moodConfidence, agemin, ageminConfidence, agemax, agemaxConfidence, ageest, ageestConfidence, gender, genderConfidence))
        self.connection.commit()

    #
    # stores face results in the DB
    #
    def storeResults(self, smiling, smilingConfidence, mood, moodConfidence):
        cursor = self.connection.cursor()
        cursor.execute('INSERT INTO facelog(isSmiling, smilingConfidence, mood, moodConfidence) values(?, ?, ?, ?)',
                        (smiling, smilingConfidence, mood, moodConfidence))
        self.connection.commit()

monitor = AudienceMonitor()
monitor.initialiseDB()
monitor.prepareCamera()
while True:
    photopath = "data/photo" + monitor.getTimestampString() + ".bmp"
    monitor.captureFrame(photopath)
    try:
        faceresults = monitor.faceDetection(photopath)
    except:
        print "Unexpected error:", sys.exc_info()[0]
    os.remove(photopath)
    sleep(POLL_FREQUENCY_SECONDS)</pre>
<br clear=all/><div style="font-size: small; padding: 0px 10px 0px 10px; border: 1px solid #ccc; color: #333; background-color: #eee;"><a href="http://dalelane.co.uk/blog/?p=2092">"Smile!"</a> was posted by <a href="http://dalelane.co.uk/blog/">Dale Lane</a> to <a href="http://dalelane.co.uk/blog/?p=2092">http://dalelane.co.uk/blog/?p=2092</a>.<br /><em>Feed footer idea <a href="http://www.43folders.com/feedfooter">nicked from 43 Folders</a> using the <a href="http://www.scratch99.com/wordpress-plugin-feedentryheader/">FeedEntryHeader WordPress plugin</a>.</em></div>]]></content:encoded>
			</item>
	</channel>
</rss>
