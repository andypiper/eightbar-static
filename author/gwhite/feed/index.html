<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Graham White &#8211; eightbar</title>
	<atom:link href="http://eightbar.co.uk/author/gwhite/feed/" rel="self" type="application/rss+xml" />
	<link>http://eightbar.co.uk</link>
	<description>Raising The Eight Bar</description>
	<lastBuildDate>Tue, 26 Jul 2016 07:41:27 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.5.3</generator>
	<item>
		<title>Tackling Cancer with Machine Learning</title>
		<link>http://eightbar.co.uk/2014/11/14/tackling-cancer-with-machine-learning/</link>
		<pubDate>Fri, 14 Nov 2014 08:29:00 +0000</pubDate>
		<dc:creator><![CDATA[Graham White]]></dc:creator>
				<category><![CDATA[Hursley]]></category>
		<category><![CDATA[Planet]]></category>
		<category><![CDATA[eightbar]]></category>
		<category><![CDATA[IBM]]></category>
		<category><![CDATA[innovation]]></category>
		<category><![CDATA[machine learning]]></category>

		<guid isPermaLink="false">http://eightbar.co.uk/?guid=251ff808175289d4d5b1534d9cb797f7</guid>
		<description><![CDATA[For a recent Hack Day at work I spent some time working with one of my colleagues, Adrian Lee, on a little side project to see if we could detect cancer cells in a biopsy image. &#160;We've only spent a couple of days on this so far but already the res... <a href="http://eightbar.co.uk/2014/11/14/tackling-cancer-with-machine-learning/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[For a recent Hack Day at work I spent some time working with one of my colleagues, Adrian Lee, on a little side project to see if we could detect cancer cells in a biopsy image. &nbsp;We've only spent a couple of days on this so far but already the results are looking very promising with each of us working on a distinctly different part of the overall idea.<br /><br />We held an open day in our department at work last month and I gave a lightening talk on the subject which you can see <a href="https://www.youtube.com/watch?v=n4pCty6LbuA">on YouTube</a>:<br /><br /><div class="separator" style="clear: both; text-align: center;"><object width="320" height="266" class="BLOGGER-youtube-video" classid="clsid:D27CDB6E-AE6D-11cf-96B8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0" data-thumbnail-src="https://ytimg.googleusercontent.com/vi/n4pCty6LbuA/0.jpg"><param name="movie" value="https://youtube.googleapis.com/v/n4pCty6LbuA&source=uds" /><param name="bgcolor" value="#FFFFFF" /><param name="allowFullScreen" value="true" /><embed width="320" height="266"  src="https://youtube.googleapis.com/v/n4pCty6LbuA&source=uds" type="application/x-shockwave-flash" allowfullscreen="true"/></object></div><br />There were a whole load of other talks given on the day that can be seen in the <a href="https://www-304.ibm.com/connections/blogs/et/entry/emerging_technology_centre_innovation_day">summary blog post</a> over on the <a href="http://ibm.com/blogs/et">ETS (Emerging Technology Services) site</a>.<br /><br /><br /><br />]]></content:encoded>
	<enclosure url="" length="" type="" />
		</item>
		<item>
		<title>Machine Learning Course</title>
		<link>http://eightbar.co.uk/2013/06/25/machine-learning-course/</link>
		<pubDate>Tue, 25 Jun 2013 20:21:00 +0000</pubDate>
		<dc:creator><![CDATA[Graham White]]></dc:creator>
				<category><![CDATA[Planet]]></category>
		<category><![CDATA[education]]></category>
		<category><![CDATA[eightbar]]></category>
		<category><![CDATA[machine learning]]></category>

		<guid isPermaLink="false">http://eightbar.co.uk/?guid=5eb04ae91447000b2648cd644c7e1872</guid>
		<description><![CDATA[Enough time has passed since I undertook the Stanford University Natural Language Processing Course&#160;for me to forget just how much hard work it was for me to start all over again. &#160;This year I decided to have a go at the coursera&#160;Machine... <a href="http://eightbar.co.uk/2013/06/25/machine-learning-course/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<div class="separator" style="clear: both; text-align: center;"><a href="https://s3.amazonaws.com/coursera/topics/ml/small-icon.hover.png" imageanchor="1" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"><img border="0" src="https://s3.amazonaws.com/coursera/topics/ml/small-icon.hover.png" /></a></div>Enough time has passed since I undertook the Stanford University <a href="http://gibbalog.blogspot.co.uk/2012/05/natural-language-processing-course.html">Natural Language Processing Course</a>&nbsp;for me to forget just how much hard work it was for me to start all over again. &nbsp;This year I decided to have a go at the <a href="https://www.coursera.org/">coursera</a>&nbsp;Machine Learning Course.<br /><br />Unlike the 12 week NLP course last year which estimated 10 hours a week and turned out to be more like 15-20 hours a week, this course was much more realistic in estimation at 10 weeks of 8 hours. &nbsp;I think I more or less hit the mark on that point spending about 1 day every week for the past 10 weeks studying machine learning - so around half the time required for the NLP course.<br /><br />The course was written and presented by <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a>&nbsp;who seems to be rather prolific and somewhat of an academic star in his fields of machine learning and artificial intelligence. &nbsp;He is one of the co-founders of the coursera site which along with their main rival, <a href="https://www.udacity.com/">Udacity</a>, have brought about the popular rise of <a href="http://en.wikipedia.org/wiki/Massive_open_online_course">Massive Open Online Learning</a>.<br /><br />The Machine Learning Course followed the same format as the NLP course from last year which I can only assume is the standard coursera format, at least for technical courses anyway. &nbsp;Each week there were 1 or two main topic areas to study which were presented in a series of videos featuring Andrew talking through a set of slides on which he's able to hand write notes for demonstration purposes, just as if you're sitting in a real lecture hall at university. &nbsp;To check your understanding of the content of the videos there are questions which must be answered on each topic against which you're graded. &nbsp;The second main component each week is a programming exercise which for the Machine Learning Course must be completed in Octave - so yet another programming language to add to your list. &nbsp;Achieving a mark of 80% or above across all the questions and programming exercises results in a course pass. &nbsp;I appear to have done that with relative ease for this course.<br /><br />The 18 topics covered were:<br /><br /><ul><li>Introduction</li><li>Linear Regression with One Variable</li><li>Linear Algebra Review</li><li>Linear Regression with Multiple Variables</li><li>Octave Tutorial</li><li>Logistic Regression</li><li>Regularisation</li><li>Neural Networks Representation</li><li>Neural Networks Learning</li><li>Advice for Applying Machine Learning</li><li>Machine Learning System Design</li><li>Support Vector Machines</li><li>Clustering</li><li>Dimensionality Reduction</li><li>Anomaly Detection</li><li>Recommender Systems</li><li>Large Scale Machine Learning</li><li>Application Example Photo OCR</li></ul>The course served as a good revision of some maths I haven't used in quite some time, lots of Linear Algebra for which you need a pretty good understanding and lots of calculus which you didn't really need to understand if all you care about is implementing the algorithms rather than working out how they're derived or proven. &nbsp;Being quite maths based, the course used matrices and vectorisation very heavily rather than using the loop structures that most of us would use as a go-to framework for writing complex algorithms. &nbsp;Again, this was some good revision as I've not programmed in this fashion for quite some time. &nbsp;You're definitely reminded of just how efficient you can make complex tasks on modern processors if you stand back from your algorithm for a bit and work out how best to utilise the hardware (via the appropriately optimised libraries) you have.<br /><br />The major thought behind the course seems to be to teach as many different algorithms as possible. &nbsp;There really is a great range. &nbsp;Starting of simply with linear algorithms and progressing right up to the current state-of-the-art Neural Networks and the ever fashionable map-reduce stuff.<br /><br />I didn't find the course terribly difficult, I'm no expert in any of the topics but have studied enough maths not to struggle with that side of things and don't struggle with programming either. &nbsp;I didn't need to use the forums or any of the other social elements offered during the course so I don't really have a feel for how others found the course. &nbsp;I can certainly imagine someone finding it a real struggle if they don't have a particularly deep background in either maths or programming.<br /><br />There was, as far as I can think right now, one (or maybe two depending on how you count) omission from the course. &nbsp;Most of the programming exercises were heavily frameworked for you in advance, you just have to fill in the gaps. &nbsp;This is great for learning the various different algorithms presented during the course but does leave a couple of areas at the end of the course you're not so confident with (aside from not really having a wide grasp of the Octave programming language). &nbsp;The omission of which I speak is that of storing and bootstrapping the models you've trained with the algorithm. &nbsp;All the exercises concentrated on training a model, storing it in memory, using it and as the program terminates then so your model disappears. &nbsp;It would have been great to have another module on the best ways to persist models between program runs, and how to continue training (bootstrap) a model that you have already persisted. &nbsp;I'll feed that thought back to Andrew when the opportunity arises over the next couple of weeks.<br /><br />The problem going forward wont so much be applying what has been offered here but working out what to apply it to. &nbsp;The range of problems that can be tackled with these techniques is mind-blowing, just look at the rise of analytics we're seeing in all areas of business and technology.<br /><br />Overall then, a really nice introduction into the world of machine learning. &nbsp;Recommended!<br /><br /><br />]]></content:encoded>
	<enclosure url="" length="" type="" />
		</item>
		<item>
		<title>Speech to Text</title>
		<link>http://eightbar.co.uk/2013/04/22/speech-to-text/</link>
		<pubDate>Mon, 22 Apr 2013 13:26:00 +0000</pubDate>
		<dc:creator><![CDATA[Graham White]]></dc:creator>
				<category><![CDATA[Planet]]></category>
		<category><![CDATA[eightbar]]></category>
		<category><![CDATA[IBM]]></category>

		<guid isPermaLink="false">http://eightbar.co.uk/?guid=df50f8a7c37ac92592ee3b3ccd1a2ee6</guid>
		<description><![CDATA[<div>Apologies to the <a href="http://www.urbandictionary.com/define.php?term=tl%3Bdr">tl;dr</a>&#160;brigade, this is going to be a long one...&#160;</div><div><br /></div><div>For a number of years I've been quietly working away with IBM research on our speech to text programme.  That is, working with a set of algorithms that ultimately produce a system capable of listening to human speech and transcribing it into text.  The concept is simple, train a system for speech to text - speech goes in, text comes out.  However, the process and algorithms to do this are extremely complicated from just about every way you look at it &#8211; computationally, mathematically, operationally, evaluationally, time and cost.  This is a completely separate topic and area of research from the similar sounding text to speech systems that take text (such as this blog) and read it aloud in a computerised voice.</div><div><br /></div><div>Whenever I talk to people about it they always appear fascinated and want to know more.  The same questions often come up.  I'm going to address some of these here in a generic way and leaving out those that I'm unable to talk about here.  I should also point out that I'm by no means a speech expert or linguist but have developed enough of an understanding to be dangerous in the subject matter and that (I hope) allows me to explain things in a way that others not familiar with the field are able to understand.  I'm deliberately not linking out to the various research topics that come into play during this post as the list would become lengthy very quickly and this isn't a formal paper after all, Internet searches are your friend if you want to know more.</div><div><br /></div><div><b>I didn't know IBM did that?</b></div><div>OK so not strictly a question but the answer is yes, we do.  We happen to be pretty good at it as well.  However, we typically use a company called Nuance as our preferred partner.</div><div><br /></div><div>People have often heard of IBM's former product in this area called Via Voice for their desktop PCs which was available until the early 2000's.  This sort of technology allowed a single user to speak to their computer for various different purposes and required the user to spend some time training the software before it would understand their particular voice.  Today's speech software has progressed beyond this to systems that don't require any training by the user before they use it.  Current systems are trained in advance in order to attempt to understand any voice.</div><div><br /></div><div><b>What's required?</b></div><div>Assuming you have the appropriate software and the hardware required to run it on then you need three more things to build a speech to text system: audio, transcripts and a phonetic dictionary of pronunciations.  This sounds quite simple but when you dig under the covers a little you realise it's much more complicated (not to mention expensive) and the devil is very much in the detail.</div><div><br /></div><div>On the audial side you'll need a set of speech recordings.  If you want to evaluate your system after it has been trained then a small sample of these should be kept to one side and not used during the training process.  This set of audio used for evaluation is usually termed the held out set.  It's considered cheating if you later evaluate the system using audio that was included in the training process &#8211; since the system has already &#8220;heard&#8221; this audio before it would have a higher chance of accurately reproducing it later.  The creation of the held out set leads to two sets of audio files, the held out set and the majority of the audio that remains which is called the training set.</div><div><br /></div><div>The audio can be in any format your training software is compatible with but wave files are commonly used.  The quality of the audio both in terms of the digital quality (e.g. sample rate) as well as the quality of the speaker(s) and the equipment used for the recordings will have a direct bearing on the resulting accuracy of the system being trained.  Simply put, the better quality you can make the input, the more accurate the output will be.  This leads to another bunch of questions such as but not limited to &#8220;What quality is optimal?&#8221;, &#8220;What should I get the speakers to say?&#8221;, &#8220;How should I capture the recordings?&#8221; - all of which are research topics in their own right and for which there is no one-size-fits-all answer.</div><div><br /></div><div>Capturing the audio is one half of the battle.  The next piece in the puzzle is obtaining well transcribed textual copies of that audio.  The transcripts should consist of a set of text representing what was said in the audio as well as some sort of indication of when during the audio a speaker starts speaking and when they stop.  This is usually done on a sentence by sentence basis, or for each utterance as they are known.  These transcripts may have a certain amount of subjectivity associated with them in terms of where the sentence boundaries are and potentially exactly what was said if the audio wasn't clear or slang terms were used.  They can be formatted in a variety of different ways and there are various standard formats for this purpose from an XML DTD through to CSV.</div><div><br /></div><div>If it has not already become clear, creating the transcription files can be quite a skilled and time consuming job.  A typical industry expectation is that it takes approximately 10 man-hours for a skilled transcriber to produce 1 hour of well formatted audio transcription.  This time plus the cost of collecting the audio in the first place is one of the factors making speech to text a long, hard and expensive process.  This is particularly the case when put into context that most current commercial speech systems are trained on at least 2000+ hours of audio with the minimum recommended amount being somewhere in the region of 500+ hours.</div><div><br /></div><div>Finally, a phonetic dictionary must either be obtained or produced that contains at least one pronunciation variant for each word said across the entire corpus of audio input.  Even for a minimal system this will run into tens of thousands of words.  There are of course, already phonetic dictionaries available such as the Oxford English Dictionary that contains a pronunciation for each word it contains.  However, this would only be appropriate for one regional accent or dialect without variation.  Hence, producing the dictionary can also be a long and skilled manual task.</div><div><br /></div><div><b>What does the software do?</b></div><div>The simple answer is that it takes audio and transcript files and passes them through a set of really rather complicated mathematical algorithms to produce a model that is particular to the input received.  This is the training process.  Once system has been trained the model it generates can be used to take speech input and produce text output.  This is the decoding process.  The training process requires lots of data and is computationally expensive but the model it produces is very small and computationally much less expensive to run.  Today's models are typically able to perform real-time (or faster) speech to text conversion on a single core of a modern CPU.  It is the model and software surrounding the model that is the piece exposed to users of the system.</div><div><br /></div><div>Various different steps are used during the training process to iterate through the different modelling techniques across the entire set of training audio provided to the trainer.  When the process first starts the software knows nothing of the audio, there are no clever boot strapping techniques used to kick-start the system in a certain direction or pre-load it in any way.  This allows the software to be entirely generic and work for all sorts of different languages and quality of material.  Starting in this way is known as a flat start or context independent training.  The software simply chops up the audio into regular segments to start with and then performs several iterations where these boundaries are shifted slightly to match the boundaries of the speech in the audio more closely.</div><div><br /></div><div>The next phase is context dependent training.  This phase starts to make the model a little more specific and tailored to the input being given to the trainer.  The pronunciation dictionary is used to refine the model to produce an initial system that could be used to decode speech into text in its own right at this early stage.  Typically, context dependent training, while an iterative process in itself, can also be run multiple times in order to hone the model still further.</div><div><br /></div><div>Another optimisation that can be made to the model after context dependent training is to apply vocal tract length normalisation.  This works on the theory that the audibility of human speech correlates to the pitch of the voice, and the pitch of the voice correlates to the vocal tract length of the speaker.  Put simply, it's a theory that says men have low voices and women  have high voices and if we normalise the wave form for all voices in the training material to have the same pitch (i.e. same vocal tract length) then audibility improves.  To do this an estimation of the vocal tract length must first be made for each speaker in the training data such that a normalisation factor can be applied to that material and the model updated to reflect the change.</div><div><br /></div><div>The model can be thought of as a tree although it's actually a large multi-dimensional matrix.  By reducing the number of dimensions in the matrix and applying various other mathematical operations to reduce the search space the model can be further improved upon both in terms of accuracy, speed and size.  This is generally done after vocal tract length normalisation has taken place.</div><div><br /></div><div>Another tweak that can be made to improve the model is to apply what we call discriminative training.  For this step the theory goes along the lines that all of the training material is decoded using the current best model produced from the previous step.  This produces a set of text files.  These text files can be compared with those produced by the human transcribers and given to the system as training material.  The comparison can be used to inform where the model can be improved and these improvements applied to the model.  It's a step that can probably be best summarised by learning from its mistakes, clever!</div><div><br /></div><div>Finally, once the model has been completed it can be used with a decoder that knows how to understand that model to produce text given an audio input.  In reality, the decoders tend to operate on two different models.  The audio model for which the process of creation has just been roughly explained; and a language model.  The language model is simply a description of how language is used in the specific context of the training material.  It would, for example, attempt to provide insight into which words typically follow which other words via the use of what natural language processing experts call n-grams.  Obtaining information to produce the language model is much easier and does not necessarily have to come entirely from the transcripts used during the training process.  Any text data that is considered representative of the speech being decoded could be useful.  For example, in an application targeted at decoding BBC News readers then articles from the BBC news web site would likely prove a useful addition to the language model.</div><div><br /></div><div><b>How accurate is it?</b></div><div>This is probably the most common question about these systems and one of the most complex to answer.  As with most things in the world of high technology it's not simple, so the answer is the infamous &#8220;it depends&#8221;.  The short answer is that in ideal circumstances the software can perform at near human levels of accuracy which equates to in excess of 90% accuracy levels.  Pretty good you'd think.  It has been shown that human performance is somewhere in excess of 90% and is almost never 100% accuracy.  The test for this is quite simple, you get two (or more) people to independently transcribe some speech and compare the results from each speaker, almost always there will be a disagreement about some part of the speech (if there's enough speech that is).</div><div><br /></div><div>It's not often that ideal circumstances are present or can even realistically be achieved.  Ideal would be transcribing a speaker with a similar voice and accent to those which have been trained into the model and they would speak at the right speed (not too fast and not too slowly) and they would use a directional microphone that didn't do any fancy noise cancellation, etc. What people are generally interested in is the real-world situation, something along the lines of &#8220;if I speak to my phone, will it understand me?&#8221;.  This sort of real-world environment often includes background noise and a very wide variety of speakers potentially speaking into a non-optimal recording device.  Even this can be a complicated answer for the purposes of accuracy.  We're talking about free, conversational style, speech in this blog post and there's a huge different in recognising any and all words versus recognising a small set of command and control words for if you wanted your phone to perform a specific action.  In conclusion then, we can only really speak about the art of the possible and what has been achieved before.  If you want to know about accuracy for your particular situation and your particular voice on your particular device then you'd have to test it!</div><div><br /></div><div><b>What words can it understand?  What about slang?</b></div><div>The range of understanding of a speech to text system is dependent on the training material.  At present, the state of the art systems are based on dictionaries of words and don't generally attempt to recognise new words for which an entry in the dictionary has not been found (although these types of systems are available separately and could be combined into a speech to text solution if necessary).  So the number and range of words understood by a speech to text system is currently (and I'm generalising here) a function of the number and range of words used in the training material.  It doesn't really matter what these words are, whether they're conversational and slang terms or proper dictionary terms, so long as the system was trained on those then it should be able to recognise them again during a decode.</div><div><br /></div><div><b>Updates and Maintenance</b></div><div>For the more discerning reader, you'll have realised by now a fundamental flaw in the plan laid out thus far.  Language changes over time, people use new words and the meaning of words changes within the language we use.  Text-speak is one of the new kids on the block in this area.  It would be extremely cumbersome to need to train an entire new model each time you wished to update your previous one in order to include some set of new language capability.  The models produced are able to be modified and updated with these changes without the need to go back to a full standing start and training from scratch all over again.  It's possible to take your existing model built from the set of data you had available at a particular point in time and use this to bootstrap the creation of a new model which will be enhanced with the new materials that you've gathered since training the first model.  Of course, you'll want to test and compare both models to check that you have in fact enhanced performance as you were expecting.  This type of maintenance and update to the model will be required to any and all of these types of systems as they're currently designed as the structure and usage of our languages evolve.</div><div><br /></div><div><b>Conclusion</b></div><div>OK, so not necessarily a blog post that was ever designed to draw a conclusion but I wanted to wrap up by saying that this is an area of technology that is still very much in active research and development, and has been so for at least 40-50 years or more!  There's a really interesting statistic I've seen in the field that says if you ask a range of people involved in this topic the answer to the question &#8220;when will speech to text become a reality&#8221; then the answer generally comes out at &#8220;in ten years time&#8221;.  This question has been asked consistently over time and the answer has remained the same.  It seems then, that either this is a really hard nut to crack or that our expectations of such a system move on over time.  Either way, it seems there will always be something new just around the corner to advance us to the next stage of speech technologies.</div> <a href="http://eightbar.co.uk/2013/04/22/speech-to-text/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<div style="margin-bottom: 0cm;">Apologies to the <a href="http://www.urbandictionary.com/define.php?term=tl%3Bdr">tl;dr</a>&nbsp;brigade, this is going to be a long one...&nbsp;</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">For a number of years I've been quietly working away with IBM research on our speech to text programme.  That is, working with a set of algorithms that ultimately produce a system capable of listening to human speech and transcribing it into text.  The concept is simple, train a system for speech to text - speech goes in, text comes out.  However, the process and algorithms to do this are extremely complicated from just about every way you look at it – computationally, mathematically, operationally, evaluationally, time and cost.  This is a completely separate topic and area of research from the similar sounding text to speech systems that take text (such as this blog) and read it aloud in a computerised voice.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">Whenever I talk to people about it they always appear fascinated and want to know more.  The same questions often come up.  I'm going to address some of these here in a generic way and leaving out those that I'm unable to talk about here.  I should also point out that I'm by no means a speech expert or linguist but have developed enough of an understanding to be dangerous in the subject matter and that (I hope) allows me to explain things in a way that others not familiar with the field are able to understand.  I'm deliberately not linking out to the various research topics that come into play during this post as the list would become lengthy very quickly and this isn't a formal paper after all, Internet searches are your friend if you want to know more.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><b>I didn't know IBM did that?</b></div><div style="margin-bottom: 0cm;">OK so not strictly a question but the answer is yes, we do.  We happen to be pretty good at it as well.  However, we typically use a company called Nuance as our preferred partner.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">People have often heard of IBM's former product in this area called Via Voice for their desktop PCs which was available until the early 2000's.  This sort of technology allowed a single user to speak to their computer for various different purposes and required the user to spend some time training the software before it would understand their particular voice.  Today's speech software has progressed beyond this to systems that don't require any training by the user before they use it.  Current systems are trained in advance in order to attempt to understand any voice.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><b>What's required?</b></div><div style="margin-bottom: 0cm;">Assuming you have the appropriate software and the hardware required to run it on then you need three more things to build a speech to text system: audio, transcripts and a phonetic dictionary of pronunciations.  This sounds quite simple but when you dig under the covers a little you realise it's much more complicated (not to mention expensive) and the devil is very much in the detail.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">On the audial side you'll need a set of speech recordings.  If you want to evaluate your system after it has been trained then a small sample of these should be kept to one side and not used during the training process.  This set of audio used for evaluation is usually termed the held out set.  It's considered cheating if you later evaluate the system using audio that was included in the training process – since the system has already “heard” this audio before it would have a higher chance of accurately reproducing it later.  The creation of the held out set leads to two sets of audio files, the held out set and the majority of the audio that remains which is called the training set.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">The audio can be in any format your training software is compatible with but wave files are commonly used.  The quality of the audio both in terms of the digital quality (e.g. sample rate) as well as the quality of the speaker(s) and the equipment used for the recordings will have a direct bearing on the resulting accuracy of the system being trained.  Simply put, the better quality you can make the input, the more accurate the output will be.  This leads to another bunch of questions such as but not limited to “What quality is optimal?”, “What should I get the speakers to say?”, “How should I capture the recordings?” - all of which are research topics in their own right and for which there is no one-size-fits-all answer.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">Capturing the audio is one half of the battle.  The next piece in the puzzle is obtaining well transcribed textual copies of that audio.  The transcripts should consist of a set of text representing what was said in the audio as well as some sort of indication of when during the audio a speaker starts speaking and when they stop.  This is usually done on a sentence by sentence basis, or for each utterance as they are known.  These transcripts may have a certain amount of subjectivity associated with them in terms of where the sentence boundaries are and potentially exactly what was said if the audio wasn't clear or slang terms were used.  They can be formatted in a variety of different ways and there are various standard formats for this purpose from an XML DTD through to CSV.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">If it has not already become clear, creating the transcription files can be quite a skilled and time consuming job.  A typical industry expectation is that it takes approximately 10 man-hours for a skilled transcriber to produce 1 hour of well formatted audio transcription.  This time plus the cost of collecting the audio in the first place is one of the factors making speech to text a long, hard and expensive process.  This is particularly the case when put into context that most current commercial speech systems are trained on at least 2000+ hours of audio with the minimum recommended amount being somewhere in the region of 500+ hours.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">Finally, a phonetic dictionary must either be obtained or produced that contains at least one pronunciation variant for each word said across the entire corpus of audio input.  Even for a minimal system this will run into tens of thousands of words.  There are of course, already phonetic dictionaries available such as the Oxford English Dictionary that contains a pronunciation for each word it contains.  However, this would only be appropriate for one regional accent or dialect without variation.  Hence, producing the dictionary can also be a long and skilled manual task.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><b>What does the software do?</b></div><div style="margin-bottom: 0cm;">The simple answer is that it takes audio and transcript files and passes them through a set of really rather complicated mathematical algorithms to produce a model that is particular to the input received.  This is the training process.  Once system has been trained the model it generates can be used to take speech input and produce text output.  This is the decoding process.  The training process requires lots of data and is computationally expensive but the model it produces is very small and computationally much less expensive to run.  Today's models are typically able to perform real-time (or faster) speech to text conversion on a single core of a modern CPU.  It is the model and software surrounding the model that is the piece exposed to users of the system.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">Various different steps are used during the training process to iterate through the different modelling techniques across the entire set of training audio provided to the trainer.  When the process first starts the software knows nothing of the audio, there are no clever boot strapping techniques used to kick-start the system in a certain direction or pre-load it in any way.  This allows the software to be entirely generic and work for all sorts of different languages and quality of material.  Starting in this way is known as a flat start or context independent training.  The software simply chops up the audio into regular segments to start with and then performs several iterations where these boundaries are shifted slightly to match the boundaries of the speech in the audio more closely.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">The next phase is context dependent training.  This phase starts to make the model a little more specific and tailored to the input being given to the trainer.  The pronunciation dictionary is used to refine the model to produce an initial system that could be used to decode speech into text in its own right at this early stage.  Typically, context dependent training, while an iterative process in itself, can also be run multiple times in order to hone the model still further.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">Another optimisation that can be made to the model after context dependent training is to apply vocal tract length normalisation.  This works on the theory that the audibility of human speech correlates to the pitch of the voice, and the pitch of the voice correlates to the vocal tract length of the speaker.  Put simply, it's a theory that says men have low voices and women  have high voices and if we normalise the wave form for all voices in the training material to have the same pitch (i.e. same vocal tract length) then audibility improves.  To do this an estimation of the vocal tract length must first be made for each speaker in the training data such that a normalisation factor can be applied to that material and the model updated to reflect the change.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">The model can be thought of as a tree although it's actually a large multi-dimensional matrix.  By reducing the number of dimensions in the matrix and applying various other mathematical operations to reduce the search space the model can be further improved upon both in terms of accuracy, speed and size.  This is generally done after vocal tract length normalisation has taken place.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">Another tweak that can be made to improve the model is to apply what we call discriminative training.  For this step the theory goes along the lines that all of the training material is decoded using the current best model produced from the previous step.  This produces a set of text files.  These text files can be compared with those produced by the human transcribers and given to the system as training material.  The comparison can be used to inform where the model can be improved and these improvements applied to the model.  It's a step that can probably be best summarised by learning from its mistakes, clever!</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">Finally, once the model has been completed it can be used with a decoder that knows how to understand that model to produce text given an audio input.  In reality, the decoders tend to operate on two different models.  The audio model for which the process of creation has just been roughly explained; and a language model.  The language model is simply a description of how language is used in the specific context of the training material.  It would, for example, attempt to provide insight into which words typically follow which other words via the use of what natural language processing experts call n-grams.  Obtaining information to produce the language model is much easier and does not necessarily have to come entirely from the transcripts used during the training process.  Any text data that is considered representative of the speech being decoded could be useful.  For example, in an application targeted at decoding BBC News readers then articles from the BBC news web site would likely prove a useful addition to the language model.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><b>How accurate is it?</b></div><div style="margin-bottom: 0cm;">This is probably the most common question about these systems and one of the most complex to answer.  As with most things in the world of high technology it's not simple, so the answer is the infamous “it depends”.  The short answer is that in ideal circumstances the software can perform at near human levels of accuracy which equates to in excess of 90% accuracy levels.  Pretty good you'd think.  It has been shown that human performance is somewhere in excess of 90% and is almost never 100% accuracy.  The test for this is quite simple, you get two (or more) people to independently transcribe some speech and compare the results from each speaker, almost always there will be a disagreement about some part of the speech (if there's enough speech that is).</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;">It's not often that ideal circumstances are present or can even realistically be achieved.  Ideal would be transcribing a speaker with a similar voice and accent to those which have been trained into the model and they would speak at the right speed (not too fast and not too slowly) and they would use a directional microphone that didn't do any fancy noise cancellation, etc. What people are generally interested in is the real-world situation, something along the lines of “if I speak to my phone, will it understand me?”.  This sort of real-world environment often includes background noise and a very wide variety of speakers potentially speaking into a non-optimal recording device.  Even this can be a complicated answer for the purposes of accuracy.  We're talking about free, conversational style, speech in this blog post and there's a huge different in recognising any and all words versus recognising a small set of command and control words for if you wanted your phone to perform a specific action.  In conclusion then, we can only really speak about the art of the possible and what has been achieved before.  If you want to know about accuracy for your particular situation and your particular voice on your particular device then you'd have to test it!</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><b>What words can it understand?  What about slang?</b></div><div style="margin-bottom: 0cm;">The range of understanding of a speech to text system is dependent on the training material.  At present, the state of the art systems are based on dictionaries of words and don't generally attempt to recognise new words for which an entry in the dictionary has not been found (although these types of systems are available separately and could be combined into a speech to text solution if necessary).  So the number and range of words understood by a speech to text system is currently (and I'm generalising here) a function of the number and range of words used in the training material.  It doesn't really matter what these words are, whether they're conversational and slang terms or proper dictionary terms, so long as the system was trained on those then it should be able to recognise them again during a decode.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><b>Updates and Maintenance</b></div><div style="margin-bottom: 0cm;">For the more discerning reader, you'll have realised by now a fundamental flaw in the plan laid out thus far.  Language changes over time, people use new words and the meaning of words changes within the language we use.  Text-speak is one of the new kids on the block in this area.  It would be extremely cumbersome to need to train an entire new model each time you wished to update your previous one in order to include some set of new language capability.  The models produced are able to be modified and updated with these changes without the need to go back to a full standing start and training from scratch all over again.  It's possible to take your existing model built from the set of data you had available at a particular point in time and use this to bootstrap the creation of a new model which will be enhanced with the new materials that you've gathered since training the first model.  Of course, you'll want to test and compare both models to check that you have in fact enhanced performance as you were expecting.  This type of maintenance and update to the model will be required to any and all of these types of systems as they're currently designed as the structure and usage of our languages evolve.</div><div style="margin-bottom: 0cm;"><br /></div><div style="margin-bottom: 0cm;"><b>Conclusion</b></div><div style="margin-bottom: 0cm;">OK, so not necessarily a blog post that was ever designed to draw a conclusion but I wanted to wrap up by saying that this is an area of technology that is still very much in active research and development, and has been so for at least 40-50 years or more!  There's a really interesting statistic I've seen in the field that says if you ask a range of people involved in this topic the answer to the question “when will speech to text become a reality” then the answer generally comes out at “in ten years time”.  This question has been asked consistently over time and the answer has remained the same.  It seems then, that either this is a really hard nut to crack or that our expectations of such a system move on over time.  Either way, it seems there will always be something new just around the corner to advance us to the next stage of speech technologies.</div>]]></content:encoded>
	<enclosure url="" length="" type="" />
		</item>
		<item>
		<title>Going Back to University</title>
		<link>http://eightbar.co.uk/2013/03/19/going-back-to-university/</link>
		<pubDate>Tue, 19 Mar 2013 21:15:00 +0000</pubDate>
		<dc:creator><![CDATA[Graham White]]></dc:creator>
				<category><![CDATA[Planet]]></category>
		<category><![CDATA[education]]></category>
		<category><![CDATA[eightbar]]></category>
		<category><![CDATA[exeter]]></category>
		<category><![CDATA[IBM]]></category>
		<category><![CDATA[university]]></category>

		<guid isPermaLink="false">http://eightbar.co.uk/?guid=d6af999ad7113d8481b3cc8c795eca7b</guid>
		<description><![CDATA[   A couple of weeks ago I had the enormous pleasure of returning to Exeter University where I studied for my degree more years ago than seems possible. &#160;Getting involved with the uni again has been something I've long since wanted to do in an att... <a href="http://eightbar.co.uk/2013/03/19/going-back-to-university/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<a href="http://www.exeter.ac.uk/media/universityofexeter/webteam/styleassets/images/logo.gif" imageanchor="1" style="margin-bottom: 1em; margin-right: 1em;"><img border="0" src="http://www.exeter.ac.uk/media/universityofexeter/webteam/styleassets/images/logo.gif" /></a>  <a href="http://upload.wikimedia.org/wikipedia/en/3/35/Exeter_University_Crest_Colour.jpg" imageanchor="1" style="float: right; margin-bottom: 1em; margin-left: 1em;"><img border="0" height="200" src="http://upload.wikimedia.org/wikipedia/en/3/35/Exeter_University_Crest_Colour.jpg" width="157" /></a> <br /><br />A couple of weeks ago I had the enormous pleasure of returning to Exeter University where I studied for my degree more years ago than seems possible. &nbsp;Getting involved with the uni again has been something I've long since wanted to do in an attempt to give back something to the institution to which I owe so much having been there to get good qualifications and not least met my wife there too! &nbsp;I think early on in a career it's not necessarily something I would have been particularly useful for since I was closer to the university than my working life in age, mentality and a bunch of other factors I'm sure. &nbsp;However, getting a bit older makes me feel readier to provide something tangibly useful in terms of giving something back both to the university and to the current students. &nbsp;I hope that having been there recently with work it's a relationship I can start to build up.<br /><br />I should probably steer clear of saying exactly why we were there but there was a small team from work some of which I knew well such as <a href="http://twitter.com/mandieq">@madieq</a> and <a href="http://twitter.com/andysc">@andysc</a>&nbsp;and one or two I hadn't come across before. &nbsp;Our job was to work with some academic staff for a couple of days and so it was a bit of a departure from my normal work with corporate customers. &nbsp;It's fantastic to see the university from the other side of the fence (i.e. not being a student) and hearing about some of the things going on there and seeing a university every bit as vibrant and ambitious as the one I left in 2000. Of course, there was the obligatory wining and dining in the evening which just went to make the experience all the more pleasurable.<br /><br />I really hope to be able to talk a lot more about things we're doing with the university in the future. &nbsp;Until then, I'm looking forward to going back a little more often and potentially imparting some words (of wisdom?) to some students too.]]></content:encoded>
	<enclosure url="" length="" type="" />
		</item>
		<item>
		<title>Hursley Celebration</title>
		<link>http://eightbar.co.uk/2012/05/31/hursley-celebration/</link>
		<pubDate>Thu, 31 May 2012 15:15:46 +0000</pubDate>
		<dc:creator><![CDATA[Graham White]]></dc:creator>
				<category><![CDATA[Hursley]]></category>
		<category><![CDATA[Life]]></category>

		<guid isPermaLink="false">http://eightbar.co.uk/?p=1351</guid>
		<description><![CDATA[Today is one of those great days in Hursley when everyone lifts their head and gets away from their desk for a little while&#8230; OK, so excuse the quality of that picture as it&#8217;s just a quick snap from my &#8230; <a href="http://eightbar.co.uk/2012/05/31/hursley-celebration/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>Today is one of those great days in Hursley when everyone lifts their head and gets away from their desk for a little while&#8230;</p>
<p><a href="http://eightbar.co.uk/wp-content/uploads/2012/05/IMAG0060.jpg"><img src="http://eightbar.co.uk/wp-content/uploads/2012/05/IMAG0060-300x179.jpg" alt="Car Fair" title="Car Fair" width="400" height="239" class="aligncenter size-medium wp-image-1352" srcset="http://eightbar.co.uk/wp-content/uploads/2012/05/IMAG0060-300x179.jpg 300w, http://eightbar.co.uk/wp-content/uploads/2012/05/IMAG0060-1024x613.jpg 1024w, http://eightbar.co.uk/wp-content/uploads/2012/05/IMAG0060-500x300.jpg 500w" sizes="(max-width: 400px) 100vw, 400px" /></a></p>
<p>OK, so excuse the quality of that picture as it&#8217;s just a quick snap from my phone.  Every few years we have a classic car fair on site, there seems to be no rhythm to when they&#8217;re held, possibly it&#8217;s just long enough since we&#8217;ve all forgotten about the cars we saw at the same show last time round &#8211; but I&#8217;m sure there are some different ones too.</p>
<p>Today&#8217;s celebration is under the guise of an Olympic celebration so in addition to the car show there&#8217;s a big quiz taking place, a careers fair, several different &#8220;sporting&#8221; events (such as egg and spoon race and the like) so it&#8217;s as much a summer fair as anything else; and it&#8217;s not raining which is always a bonus.  The real draw of course is the free cookie or scone and drink of course, but however you look at it, to have these sorts of events on site (and such a lovely site on a summers day) is absolutely brilliant.  It&#8217;s a great chance for us all to take a little time away from the desk in the afternoon, catch up with friends, see what&#8217;s going on while enjoying ourselves and having a bit of fun.</p>
<p>&lt;edit&gt;More pictures are coming in of the event on Twitter&#8230;&lt;/edit&gt;<br />
Reproduced <a href="https://twitter.com/sjmaple/status/208511582249037824">with permission</a> from <a href="http://twitter.com/sjmaple">Simon Maple</a><br />
<a href="https://twitter.com/sjmaple/status/208209636812275712/photo/1"><img alt="Delorean at Hursley by Simon Maple" src="https://p.twimg.com/AuO1j8FCQAIoo-S.jpg:large" title="Delorean at Hursley by Simon Maple" class="aligncenter" width="484" height="648" /></a></p>
<p>Reproduced <a href="https://twitter.com/anghelides/status/208614053747240960">with permission</a> from <a href="http://twitter.com/anghelides">Peter Anghelides</a><br />
<a href="http://twitter.com/anghelides/status/208210995393470464/photo/1"><img alt="Looks like the IBM Hursley car park is full again" src="http://p.twimg.com/AuO2zBMCIAEc3Wm.jpg" title="Looks like the IBM Hursley car park is full again" class="aligncenter" width="400" height="300" /></a></p>
]]></content:encoded>
			</item>
		<item>
		<title>Natural Language Processing Course</title>
		<link>http://eightbar.co.uk/2012/05/27/natural-language-processing-course/</link>
		<pubDate>Sun, 27 May 2012 10:32:00 +0000</pubDate>
		<dc:creator><![CDATA[Graham White]]></dc:creator>
				<category><![CDATA[Planet]]></category>
		<category><![CDATA[education]]></category>
		<category><![CDATA[eightbar]]></category>
		<category><![CDATA[nlp]]></category>

		<guid isPermaLink="false">http://eightbar.co.uk/?guid=48330785a010e44bd60575c3ca23b86f</guid>
		<description><![CDATA[<div><a href="http://dalelane.co.uk/blog/post-images/120525-nlpclass.png"><img border="0" height="135" src="http://dalelane.co.uk/blog/post-images/120525-nlpclass.png" width="320"/></a></div><br />Over the first few months of this year I have been taking part in a mass <a href="http://www.nlp-class.org/">online learning course in Natural Language Processing</a> (NLP) run by Stanford University. &#160;They publicised a group of eight courses at the end of last year and I didn't hesitate to sign up to the Natural Language Processing course knowing it would fit very well with things I'm working on in my professional role where I'm doing more and more with text analytics and continuing my work in speech to text. &#160;There were others I could easily have signed up for too, things like security or machine learning, more or less all of them are relevant for something I'm doing. &#160;However, given the time commitment required I decided to fully commit to one course and the NLP one was to be it.<br /><br />I passed the course with a grade of 85% which was well above the required 70% pass mark. &#160;However, the effort and time required to get there was way more than I was expecting and quite a lot more than the expected time the lecturers (<a href="http://nlp.stanford.edu/~manning/">Chris Manning</a> and <a href="http://www.stanford.edu/~jurafsky/">Dan Jurafsky</a>) had said. &#160;From memory it was an 8 week course with 10 hours a week required effort to complete the work. As it went on the amount of time required went up significantly, so rather than the 80 hours total I think I spent more like 1&#38;frac12; times that at over 120 hours!<br /><br />There were four of us at work (that I know of) who embarked on the course but due to the commitment of time I've mentioned above only myself and <a href="http://dalelane.co.uk/blog/">Dale</a> finished. &#160;By the way, Dale has written an <a href="http://dalelane.co.uk/blog/?p=2162">excellent post on the structure and content of the course</a> so I'd suggest reading his blog for more details on that stuff, there's little point in me re-posting it as he's written such a good summary.<br /><br />In terms of the participants on the course, it seems to have been quite a success for Stanford University - this is the first time they have run courses in this way it seems. &#160;The lecturers gave us some statistics at a couple of strategic points throughout the course and it seems there were around 40,000 people registering an interest, of which around 5000 were watching the lecture material and around 2000 completed the course having taken part in the homework assignments.<br /><br />I'm glad I committed as much as I did. &#160;If I were one of the 5000 just watching the lectures and not doing the homework material I don't think I would have got as much out of it, but the added time required to complete the homework was significant so perhaps there's a trade-off here? &#160;It's certainly the first time I've committed this much of my own personal time (it took over the lives of myself and Dale for quite a few weeks) as I was too busy at work to spend many business hours working on the course so it was all done in evenings and weekends. &#160;That's certainly one piece of feedback I gave at the end of the course, Stanford could make the course timing more flexible but also allow more time for the course to be completed.<br /><br />My experience with the way the assignments were marked was a little different to the way Dale has described in his post. &#160;I was already very familiar with the concepts of test, development and held-out sets (three different sets of data used when training NLP systems) so wasn't surprised to see that the modules in the course didn't necessarily have an exact answer to them or more precisely that the code your wrote to perfectly analyse some data on your local system may not get full marks as it was marked against a different data set. &#160;This may seem unfair but is common practice in all NLP system training that I know of.<br /><br />All in all, an excellent course that I'm glad I did. &#160;From what I hear of the other courses, they're not as deeply involved as the NLP course so I may well give another one a go in the future but for now I need to get a little of my life back and have a well earned rest from education. <a href="http://eightbar.co.uk/2012/05/27/natural-language-processing-course/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<div class="separator" style="clear: both; text-align: center;"><a href="http://dalelane.co.uk/blog/post-images/120525-nlpclass.png" imageanchor="1" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"><img border="0" height="135" src="http://dalelane.co.uk/blog/post-images/120525-nlpclass.png" width="320" /></a></div><br />Over the first few months of this year I have been taking part in a mass <a href="http://www.nlp-class.org/">online learning course in Natural Language Processing</a> (NLP) run by Stanford University. &nbsp;They publicised a group of eight courses at the end of last year and I didn't hesitate to sign up to the Natural Language Processing course knowing it would fit very well with things I'm working on in my professional role where I'm doing more and more with text analytics and continuing my work in speech to text. &nbsp;There were others I could easily have signed up for too, things like security or machine learning, more or less all of them are relevant for something I'm doing. &nbsp;However, given the time commitment required I decided to fully commit to one course and the NLP one was to be it.<br /><br />I passed the course with a grade of 85% which was well above the required 70% pass mark. &nbsp;However, the effort and time required to get there was way more than I was expecting and quite a lot more than the expected time the lecturers (<a href="http://nlp.stanford.edu/~manning/">Chris Manning</a> and <a href="http://www.stanford.edu/~jurafsky/">Dan Jurafsky</a>) had said. &nbsp;From memory it was an 8 week course with 10 hours a week required effort to complete the work. As it went on the amount of time required went up significantly, so rather than the 80 hours total I think I spent more like 1½ times that at over 120 hours!<br /><br />There were four of us at work (that I know of) who embarked on the course but due to the commitment of time I've mentioned above only myself and <a href="http://dalelane.co.uk/blog/">Dale</a> finished. &nbsp;By the way, Dale has written an <a href="http://dalelane.co.uk/blog/?p=2162">excellent post on the structure and content of the course</a> so I'd suggest reading his blog for more details on that stuff, there's little point in me re-posting it as he's written such a good summary.<br /><br />In terms of the participants on the course, it seems to have been quite a success for Stanford University - this is the first time they have run courses in this way it seems. &nbsp;The lecturers gave us some statistics at a couple of strategic points throughout the course and it seems there were around 40,000 people registering an interest, of which around 5000 were watching the lecture material and around 2000 completed the course having taken part in the homework assignments.<br /><br />I'm glad I committed as much as I did. &nbsp;If I were one of the 5000 just watching the lectures and not doing the homework material I don't think I would have got as much out of it, but the added time required to complete the homework was significant so perhaps there's a trade-off here? &nbsp;It's certainly the first time I've committed this much of my own personal time (it took over the lives of myself and Dale for quite a few weeks) as I was too busy at work to spend many business hours working on the course so it was all done in evenings and weekends. &nbsp;That's certainly one piece of feedback I gave at the end of the course, Stanford could make the course timing more flexible but also allow more time for the course to be completed.<br /><br />My experience with the way the assignments were marked was a little different to the way Dale has described in his post. &nbsp;I was already very familiar with the concepts of test, development and held-out sets (three different sets of data used when training NLP systems) so wasn't surprised to see that the modules in the course didn't necessarily have an exact answer to them or more precisely that the code your wrote to perfectly analyse some data on your local system may not get full marks as it was marked against a different data set. &nbsp;This may seem unfair but is common practice in all NLP system training that I know of.<br /><br />All in all, an excellent course that I'm glad I did. &nbsp;From what I hear of the other courses, they're not as deeply involved as the NLP course so I may well give another one a go in the future but for now I need to get a little of my life back and have a well earned rest from education.]]></content:encoded>
	<enclosure url="" length="" type="" />
		</item>
		<item>
		<title>Hursley Emerging Tech on the News</title>
		<link>http://eightbar.co.uk/2012/05/10/hursley-emerging-tech-on-the-news/</link>
		<pubDate>Thu, 10 May 2012 17:01:30 +0000</pubDate>
		<dc:creator><![CDATA[Graham White]]></dc:creator>
				<category><![CDATA[Hursley]]></category>

		<guid isPermaLink="false">http://eightbar.co.uk/?p=1340</guid>
		<description><![CDATA[Kevin Brown who also featured in my previous eightbar post appears to be increasing his level of fame after appearing on Channel 4 news last night. Kevin has done a lot of work with HCI (Human Computer Interfaces) and is &#8230; <a href="http://eightbar.co.uk/2012/05/10/hursley-emerging-tech-on-the-news/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p><a href="http://twitter.com/kevinxbrown">Kevin Brown</a> who also featured in my <a href="http://eightbar.co.uk/2012/05/01/ets-interviews/">previous eightbar post</a> appears to be increasing his level of fame after <a href="http://www.channel4.com/news/catch-up/display/playlistref/090512/clipid/090512_4on_cars_09">appearing on Channel 4 news</a> last night.</p>
<p>Kevin has done a lot of work with HCI (Human Computer Interfaces) and is leading the way in the Hursley Emerging Technology Services department.  He has a huge interest and wealth of knowledge on the topic but the bleeding-edge HCI device catching people&#8217;s attention again at the moment is the brain reading headset from <a href="http://www.emotiv.com/">Emoviv</a> Technologies.  Kevin has been working with this device for quite some time already, having for example <a href="http://kevinxbrown.blogspot.co.uk/2010/10/emotiv-headset-and-locked-in-syndrome.html">used it with hospital patients</a>, and a wealth of other uses too including driving cars.  This gives a good indicator to how far ahead of the curve our emerging tech team can be at times.</p>
<p>The Channel 4 news clip focuses on using the headset to drive cars and puts this in the context of Google&#8217;s self-drive car too, here&#8217;s the video:</p>
<p><object id="flashObj" width="370" height="260" classid="clsid:D27CDB6E-AE6D-11cf-96B8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=9,0,47,0"><param name="movie" value="http://c.brightcove.com/services/viewer/federated_f9?isVid=1" /><param name="bgcolor" value="#FFFFFF" /><param name="flashVars" value="videoId=1630531391001&#038;playerID=69900095001&#038;playerKey=AQ~~,AAAAAEabvr4~,Wtd2HT-p_VhJQ6tgdykx3j23oh1YN-2U&#038;domain=embed&#038;dynamicStreaming=true" /><param name="base" value="http://admin.brightcove.com" /><param name="seamlesstabbing" value="false" /><param name="allowFullScreen" value="true" /><param name="swLiveConnect" value="true" /><param name="allowScriptAccess" value="always" /><embed src="http://c.brightcove.com/services/viewer/federated_f9?isVid=1" bgcolor="#FFFFFF" flashVars="videoId=1630531391001&#038;playerID=69900095001&#038;playerKey=AQ~~,AAAAAEabvr4~,Wtd2HT-p_VhJQ6tgdykx3j23oh1YN-2U&#038;domain=embed&#038;dynamicStreaming=true" base="http://admin.brightcove.com" name="flashObj" width="370" height="260" seamlesstabbing="false" type="application/x-shockwave-flash" allowFullScreen="true" swLiveConnect="true" allowScriptAccess="always" pluginspage="http://www.macromedia.com/shockwave/download/index.cgi?P1_Prod_Version=ShockwaveFlash"></embed></object></p>
]]></content:encoded>
			</item>
		<item>
		<title>Emerging Technology Services Interviews</title>
		<link>http://eightbar.co.uk/2012/05/01/ets-interviews/</link>
		<comments>http://eightbar.co.uk/2012/05/01/ets-interviews/#comments</comments>
		<pubDate>Tue, 01 May 2012 07:38:23 +0000</pubDate>
		<dc:creator><![CDATA[Graham White]]></dc:creator>
				<category><![CDATA[Hursley]]></category>
		<category><![CDATA[Technology]]></category>

		<guid isPermaLink="false">http://eightbar.co.uk/?p=1328</guid>
		<description><![CDATA[The British Computer Society recently came to Hursley to interview some of the members of Emerging Technology Services about some of the work we&#8217;ve been doing recently. The results, as ever in ETS, are really interesting so here is the &#8230; <a href="http://eightbar.co.uk/2012/05/01/ets-interviews/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>The <a href="http://www.bcs.org/">British Computer Society</a> recently came to Hursley to interview some of the members of Emerging Technology Services about some of the work we&#8217;ve been doing recently.  The results, as ever in ETS, are really interesting so here is the set of video interviews reposted for all you Eightbar subscribers out there. </p>
<p>To kick things off we have <a href="http://twitter.com/BharatBedi">Bharat Bedi</a>, IBM Master Inventor, talking about his work on the Universal Information Framework.  This is an innovative idea that allows secure interactions that could benefit, for example, banks:</p>
<p><embed src="http://c.brightcove.com/services/viewer/federated_f8/979266267" bgcolor="#FFFFFF" flashVars="videoId=1572786655001&#038;playerId=979266267&#038;viewerSecureGatewayURL=https://console.brightcove.com/services/amfgateway&#038;servicesURL=http://services.brightcove.com/services&#038;cdnURL=http://admin.brightcove.com&#038;domain=embed&#038;autoStart=false&#038;" base="http://admin.brightcove.com" name="flashObj" width="486" height="412" seamlesstabbing="false" type="application/x-shockwave-flash" swLiveConnect="true" pluginspage="http://www.macromedia.com/shockwave/download/index.cgi?P1_Prod_Version=ShockwaveFlash"></embed></p>
<p>Another piece from <a href="http://twitter.com/BharatBedi">Bharat Bedi</a> but this time talking about his work on the Living Safe project which runs in Balzano, Italy to help older residents who live by themselves:</p>
<p><embed src="http://c.brightcove.com/services/viewer/federated_f8/979266267" bgcolor="#FFFFFF" flashVars="videoId=1572493737001&#038;playerId=979266267&#038;viewerSecureGatewayURL=https://console.brightcove.com/services/amfgateway&#038;servicesURL=http://services.brightcove.com/services&#038;cdnURL=http://admin.brightcove.com&#038;domain=embed&#038;autoStart=false&#038;" base="http://admin.brightcove.com" name="flashObj" width="486" height="412" seamlesstabbing="false" type="application/x-shockwave-flash" swLiveConnect="true" pluginspage="http://www.macromedia.com/shockwave/download/index.cgi?P1_Prod_Version=ShockwaveFlash"></embed></p>
<p>Now something a little different from <a href="http://twitter.com/kevinxbrown">Kevin Brown</a>, IBM Senior Inventor, talking about his work using a mind-reading headset.  Here he gets Brian Runciman from the BCS to drive a car with his brain and trains him to run a brain wave reading headset:</p>
<p><embed src="http://c.brightcove.com/services/viewer/federated_f8/979266267" bgcolor="#FFFFFF" flashVars="videoId=1572458597001&#038;playerId=979266267&#038;viewerSecureGatewayURL=https://console.brightcove.com/services/amfgateway&#038;servicesURL=http://services.brightcove.com/services&#038;cdnURL=http://admin.brightcove.com&#038;domain=embed&#038;autoStart=false&#038;" base="http://admin.brightcove.com" name="flashObj" width="486" height="412" seamlesstabbing="false" type="application/x-shockwave-flash" swLiveConnect="true" pluginspage="http://www.macromedia.com/shockwave/download/index.cgi?P1_Prod_Version=ShockwaveFlash"></embed></p>
<p>Next up we have <a href="http://twitter.com/domharries">Dominic Harries</a>, IBM Emerging Technologies Specialist, talking about some of his work using a multi-user multi-touch surface.  Here Dominic is demonstrating the use of a business application on the multi-touch table:</p>
<p><embed src="http://c.brightcove.com/services/viewer/federated_f8/979266267" bgcolor="#FFFFFF" flashVars="videoId=1569333001001&#038;playerId=979266267&#038;viewerSecureGatewayURL=https://console.brightcove.com/services/amfgateway&#038;servicesURL=http://services.brightcove.com/services&#038;cdnURL=http://admin.brightcove.com&#038;domain=embed&#038;autoStart=false&#038;" base="http://admin.brightcove.com" name="flashObj" width="486" height="412" seamlesstabbing="false" type="application/x-shockwave-flash" swLiveConnect="true" pluginspage="http://www.macromedia.com/shockwave/download/index.cgi?P1_Prod_Version=ShockwaveFlash"></embed></p>
<p>Last, but not least we have <a href="http://twitter.com/helenbowyer">Helen Bowyer</a>, Emerging Technologies Manager, talking about her work on Automatic Sign Language.  Helen explains and demonstrates the Say It, Sign It (SiSi) project which uses an avatar to translate spoken English into sign language.</p>
<p><embed src="http://c.brightcove.com/services/viewer/federated_f8/979266267" bgcolor="#FFFFFF" flashVars="videoId=1569298916001&#038;playerId=979266267&#038;viewerSecureGatewayURL=https://console.brightcove.com/services/amfgateway&#038;servicesURL=http://services.brightcove.com/services&#038;cdnURL=http://admin.brightcove.com&#038;domain=embed&#038;autoStart=false&#038;" base="http://admin.brightcove.com" name="flashObj" width="486" height="412" seamlesstabbing="false" type="application/x-shockwave-flash" swLiveConnect="true" pluginspage="http://www.macromedia.com/shockwave/download/index.cgi?P1_Prod_Version=ShockwaveFlash"></embed></p>
<p>The original content can be found at <a href="http://www.bcs.org/content/conWebDoc/44430">http://www.bcs.org/content/conWebDoc/44430</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://eightbar.co.uk/2012/05/01/ets-interviews/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>Failing to Invent</title>
		<link>http://eightbar.co.uk/2012/03/01/failing-to-invent/</link>
		<pubDate>Thu, 01 Mar 2012 12:42:00 +0000</pubDate>
		<dc:creator><![CDATA[Graham White]]></dc:creator>
				<category><![CDATA[Hursley]]></category>
		<category><![CDATA[Planet]]></category>
		<category><![CDATA[eightbar]]></category>
		<category><![CDATA[IBM]]></category>
		<category><![CDATA[innovation]]></category>

		<guid isPermaLink="false">http://eightbar.co.uk/?guid=1265eac8e2b29f119b28f462b193f965</guid>
		<description><![CDATA[We IBM employees are encouraged, indeed incented, to be innovative and to invent. &#160;This is particularly poignant for people like myself working on the leading edge of the latest technologies. &#160;I work in IBM emerging technologies which is all ... <a href="http://eightbar.co.uk/2012/03/01/failing-to-invent/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[We IBM employees are encouraged, indeed incented, to be innovative and to invent. &nbsp;This is particularly poignant for people like myself working on the leading edge of the latest technologies. &nbsp;I work in IBM emerging technologies which is all about taking the latest available technology to our customers. &nbsp;We do this in a number of different ways but that's a blog post in itself. &nbsp;Innovation is often confused for or used interchangeably with invention but they are different, invention for IBM means patents, patenting and the patent process. &nbsp;That is, if I come up with something inventive I'm very much encouraged to protect that idea using patents and there are processes and help available to allow me to do that.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://xkcd.com/626/"><img border="0" height="117" src="http://imgs.xkcd.com/comics/newton_and_leibniz.png" width="400" /></a></div><br />This comic strip really sums up what can often happen when you investigate protecting one of your ideas with a patent. &nbsp;It struck me recently while out to dinner with friends that there's nothing wrong with failing to invent as the cartoon above says Leibniz did. &nbsp;It's the innovation that's important here and unlucky for Leibniz that he wasn't seen to be inventing. &nbsp;It can be quite difficult to think of something sufficiently new that it is patent-worthy and this often happens to me and those I work with while trying to protect our own ideas.<br /><br />The example I was drawing upon on this occasion was an idea I was discussing at work with some colleagues about a certain usage of your mobile phone [<i>I'm being intentionally vague here</i>]. &nbsp;After thinking it all through we came to the realisation that while the idea was good and the solution innovative, all the technology was already known available and assembled in the way we were proposing, but used somewhere completely different.<br /><br />So, failing to invent is no bad thing.&nbsp; We tried and on this particular occasion decided we could innovate but not invent.&nbsp; Next time things could be the other way around but according to these definitions we shouldn't be afraid to innovate at the price of invention anyway.]]></content:encoded>
	<enclosure url="" length="" type="" />
		</item>
		<item>
		<title>Where are they now?</title>
		<link>http://eightbar.co.uk/2012/02/23/where-are-they-now/</link>
		<pubDate>Thu, 23 Feb 2012 19:25:16 +0000</pubDate>
		<dc:creator><![CDATA[Graham White]]></dc:creator>
				<category><![CDATA[Life]]></category>

		<guid isPermaLink="false">http://eightbar.co.uk/?p=1273</guid>
		<description><![CDATA[Ian Hughes/Epredator As part of the reorganisation of the Eightbar site recently I&#8217;ve been catching up with some of the honored past Eightbar members. We say past in the loosest sense of course, Eightbar was set up with the principle &#8230; <a href="http://eightbar.co.uk/2012/02/23/where-are-they-now/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p><img src="http://www.feedingedge.co.uk/blog/wp-content/themes/atahualpa/images/epredator.jpg" alt="Ian Hughes" /> <i><a href="http://www.feedingedge.co.uk/blog/epredator-bio/">Ian Hughes/Epredator</a></i></p>
<p>As part of the reorganisation of the Eightbar site recently I&#8217;ve been catching up with some of the honored past Eightbar members.  We say past in the loosest sense of course, Eightbar was set up with the principle that &#8220;Once you&#8217;re Eightbar, you&#8217;re always Eightbar&#8221;.  Here, I manage to muscle in on some of Ian Hughes&#8217; (a.k.a epredator) time as he&#8217;s kindly answered some questions for us.  What follows is a 10 question interview style post where I talk to Ian about life after IBM &#8211; in more than 140 characters.  I think it&#8217;s a really interesting read, enjoy&#8230;!</p>
<p>Ian, you worked for IBM for a long time (somewhere around 20 years!) before making the big decision to leave and form your own start-up at <a href="http://www.feedingedge.co.uk/">Feeding Edge</a> nearly 3 years ago!</p>
<p>1. What have you found are the main things keeping you busy now?</p>
<blockquote><p>Just as when I was at IBM my work life is very varied. Living and working with technology and social changes, and being a bit of a polymath I find myself mixing a lot of skills. </p>
<p>Sometimes I am coding or combining code, usually on open source platforms but often in Unity3d. Building some game elements for a startup. Other times I am on the conference circuit helping<br />
people to see the future by showing examples of how various things have changed already and how they link together to form a disruptive future. i.e. carrying on as an evangelist.</p>
<p>Much of this is still related to virtual worlds because they form a social and technical glue that still surprises many people only just getting to grips with Twitter and Facebook.</p></blockquote>
<p>2. We&#8217;ve seen your continued rise to stardom on the ITV programme <a href="http://www.itv.com/citvonline/coolstuffcollective/">The Cool Stuff Collective</a>, how did that come about?</p>
<blockquote><p>Stardom is a very strong word <img src="https://s.w.org/images/core/emoji/72x72/1f642.png" alt="🙂" class="wp-smiley" style="height: 1em; max-height: 1em;" /> It was an ambition I had tucked away to do some more TV work. Like many things though it was serendipity that brought that about. </p>
<p>As I still blog many of my ideas and things about interesting advances many of my friends still read that. A good friend and IBMer Scotty (Kevin Scott / <a href="http://twitter.com/starbase37">@starbase37</a>) had told his friend John Marley / <a href="http://twitter.com/marleyman007">@marleyman007</a> who runs a TV production company Archie Productions about all the stuff I was talking about. Games, 3d printing, virtual worlds etc. So we got connected and had a meeting about a new show John was looking to start. </p>
<p>The aim of the meeting was really a friendly catchup and for me to give John a list of things that he could put on his show. Somewhere in the conversation he said &#8220;and then you will come on set and explain that to camera and the other presenter?&#8221; Which I still thought he meant he wanted me to be tech advisor for the kids show. Then it clicked and I realised I was being thrown in at the deep end. It was one of the few shows ITV/CITV has commissioned over the past few years. </p>
<p>So really because I have always shared what I know, used the web and social media to explain and offer a kind of open source advise I ended up with a character and role on the show. Which we have done 3 series of too!</p>
<p>Cue Showreels <img src="https://s.w.org/images/core/emoji/72x72/1f642.png" alt="🙂" class="wp-smiley" style="height: 1em; max-height: 1em;" /> <a href="http://www.feedingedge.co.uk/blog/tv-showreel/">TV Showreel</a></p></blockquote>
<p><iframe width="560" height="315" src="http://www.youtube.com/embed/nYBBXLjMnlo" frameborder="0" allowfullscreen></iframe></p>
<p>3. You must enjoy being the CSC resident g33k and teaching the viewers, what do you learn from them?</p>
<blockquote><p>It has been the most fun and rewarding thing I have done. The third series in particular we moved from a studio and just the crew to being on location with schools in a Top Gear style. Whilst we were making a show for a mass audience it became even more important to be able to reach kids directly. I learned, and re-learned that the willingness to go with the flow on some ideas because they just are cool is still a magical thing. The things I say on the show are the same things I say in boardrooms and at conferences. The kids put many adults to shame though in not worrying straight away about ROI or marketing blurb. They get the idea and then fly with it.</p>
<p>It was also great to be able to reclaim geek/g33k. In a few schools the kids who were the tech geeks were suddenly allowed to be cool too. After all there was a bloke off the telly they could talk to.</p>
<p>We always had questions at the end of my future tech slot and I often didn&#8217;t get to know what they were up front, they were their questions and they were always taking me by surprise with their new angles or just the depth of understanding they showed. Once again putting many adults to shame.</p></blockquote>
<p>4. Your time on Eightbar was mainly filled with Virtual Worlds work, what&#8217;s going down with the 3D Internet now, has it progressed as you thought?</p>
<blockquote><p>It&#8217;s interesting as in many ways parts of the metaverse are now so mainstream, yet still not so much in the &#8220;business&#8221; world as you may have expected. We know that people tend to have to evolve through things, hence the struggling to understand the power of connection in social media is still a struggle for many decision makers in business. In a time of global recession with restricted travel it seems that the obvious use for communication and understanding via virtual environments is still not being exploited. Much of this is due to people being risk averse when they think their jobs are on the line. I find that many of the things we do and talk about are still reaching an audience who then say wow I didn&#8217;t think of it like that.</p>
<p>When they are used in their various forms they have a huge impact. Imperial College have some of the best examples, even with just a simple Opensim environment to help people plan a particular event it showed up real world procedures needed fixing after the first 5 minutes which saved more than money.</p>
<p>Lots of companies have floundered who where virtual world providers, but equally lots of their code is now open source. At the same time though lots of the games industry has been turned on its head by the arrival of minecraft. Which is a &#8220;game&#8221; but that uses co creation tools live in the environment. It has done a lot to help the games industry (who also did not understand virtual worlds of this sort) to look and say &#8220;oh! thats what its all about&#8221;. </p>
<p>So none of it has gone away. It hit the usual Gartner trough of dissillusionment after the confused hype and now is ploughing up the right slope. </p>
<p>Regular business will get hit with a minecraft moment though. A game changer in the same way open source software hit the IT industry, or Amazon hit retail. It&#8217;s just about being prepared to go with it when it arrives. </p>
<p>Another great development has been the ability to self build game tech environments with products like unity3d (a huge nod to <a href="http://twitter.com/robsmart">Rob Smart</a> for spotting unity3d way back too!) and have socket servers like photon and smartfoxserver. </p>
<p>I should also mention gamification, a horrible word, another thing for people to misunderstand, yet it covers the principles of applying both gaming and game technology into places it has not been before. It is often used in a lazy fashion slapping badges on things and giving out points, however at its heart the elements of playing with identity and expression online with a virtual environment in a business context provide way more benefit.</p></blockquote>
<p>5. What has the past 3 years done for 3d printing, another of your interest areas?</p>
<blockquote><p>3d printing has gone from strength to strength. It is appearing in more places and often more people have seen something about it when I talk about it. It is linked to the virtual worlds work as when you consider that a virtual environment is often about distributing digital assets from one place to another, you bolt a 3d printer on the end of that and you get digital design and distribution of physical product and the world changes.</p>
<p>The increase in open source builds like the RepRap make the hobby end of this accessible (around £400 of bits to build one). Makerbot provide some very cheap, but clever printers too that were featured heavily at CES 2012 (Consumer Elecrtonics Show) note the Consumer in that <img src="https://s.w.org/images/core/emoji/72x72/1f642.png" alt="🙂" class="wp-smiley" style="height: 1em; max-height: 1em;" /> ! Services that print for you, like Shapeways, initially funded by Phillips, have grown and moved to New York. </p>
<p>It is still something that when someone has never seen it they think it is witchcraft, somewhat like google used to seem to people <img src="https://s.w.org/images/core/emoji/72x72/1f642.png" alt="🙂" class="wp-smiley" style="height: 1em; max-height: 1em;" /> That magic is nice to share, but then applying the extrapolation of the change to the entire world economy and manufacturing business as it moves on then scares and excites in equal measure.</p></blockquote>
<p>6. What would you like to see Eightbar doing more/less of after the departure of <a href="http://twitter.com/andypiper">Andy Piper</a> from Hursley recently?</p>
<blockquote><p>When we all set up eightbar it was an antidote to the west coast US tech bloggers getting all the kudos. We&#8217;re doing some great things over here too <img src="https://s.w.org/images/core/emoji/72x72/1f642.png" alt="🙂" class="wp-smiley" style="height: 1em; max-height: 1em;" /> Just as tech blogging has evolved I would love to see eightbar carrying on as a mini brand and a voice of that same attitude wherever it needs to be.</p></blockquote>
<p>7. Looking back at IBM, any regrets about leaving?  Things you miss?</p>
<blockquote><p>I miss all the people, well nearly all <img src="https://s.w.org/images/core/emoji/72x72/1f609.png" alt="😉" class="wp-smiley" style="height: 1em; max-height: 1em;" /> Though in reality much of the work was with people all over the world having a base of people in the same timezone and same place eating lunch in the same canteen provides an anchor. As does having to battle the same corporate resilience to change, or political short sightedness. There are still a great many sparky, slightly subversive but for the right reasons, renegade thought leaders under the radar at IBM. </p>
<p>Oh and the regular pay <img src="https://s.w.org/images/core/emoji/72x72/1f642.png" alt="🙂" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p></blockquote>
<p>8. What&#8217;s been the best thing about moving on?</p>
<blockquote><p>Diversity of experiences and freedom to explore them. Like the TV work, it was just because of being open minded and master of my own calendar. I like to link everything, let one piece of work and ideas flow with another. That is tricky in a billable utilisation environment when you are not in control of the finances and the workload. It is why big corporations will keep getting side swiped by very small fast moving organisations with huge world connectivity at their finger tips. </p>
<p>I have also had to learn a lot about the various forms and processes needed to run even the smallest Ltd company. It&#8217;s an odd and archaic system, but they are the rules <img src="https://s.w.org/images/core/emoji/72x72/1f642.png" alt="🙂" class="wp-smiley" style="height: 1em; max-height: 1em;" /> It has also been fun picking various ideas and developing them getting people with the money to get interested. It gets all very Dragon&#8217;s den. </p>
<p>Freedom also allows me to try and pick things based on if I think they are beneficial in some way, not purely just because they are there. I have always prided myself on trying to act honourably in everything and with positive principles. So now it is up to me to stick to that and help others try and do the same.</p></blockquote>
<p>9. Your personal life and work-life balance must have adjusted, what does a day in the life of epredator look like now you&#8217;re self-employed?</p>
<blockquote><p>Aha! I called myself self employed once and my accountant was quick to point out I am not <img src="https://s.w.org/images/core/emoji/72x72/1f642.png" alt="🙂" class="wp-smiley" style="height: 1em; max-height: 1em;" /> This is part of what I was saying about companies and rules. As Feeding Edge is a limited company it is a legal entity in its own right that I happen to be a director of. At the same time there is a person on its official payroll, an employee… me <img src="https://s.w.org/images/core/emoji/72x72/1f642.png" alt="🙂" class="wp-smiley" style="height: 1em; max-height: 1em;" /> So as many twist and turns in business language as in any piece of tech <img src="https://s.w.org/images/core/emoji/72x72/1f642.png" alt="🙂" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>My day is much more thinly sliced than ever. I get up check a few streams of information, spot anything urgent, then do the school run, back home for 45 minute workout on UFC trainer on the Kinect, do some calls afterwards whilst cooling down. Most of the day is spent talking to the US and or my other biz partners around the gaming startup we have, building some code, pitching how bizarre the idea is. This is usually interspersed with some contacts from previous conferences getting in touch or some BCS animation and Games Dev SG business. Several times a month I pop along to a convention or meeting to talk about Tech and usual with Cool Stuff Collective as a backdrop. So the cycle continues. </p>
<p>Then there are the ad hoc conversations around other possible TV shows, or helping other startup businesses who are focussed using new tech with some connections or ideas. </p>
<p>Evenings are mix of cooking for the family, putting the kids(predlets) to bed, some gaming, heading to a Choi Kwang Do class or late night calls with US west coast for an interview or in Second Life. </p>
<p>However there is not start or end to a working day, a tweet on the way back form the school run may lead to something as much as a scheduled Skype call at 2pm. The emphasis is still on talking and sharing online.</p></blockquote>
<p>10. Finally, give us a plug for Feeding Edge, who might I be if I were your customer and what might you be able to do for me?</p>
<blockquote><p>Feeding Edge is a vehicle for people to get help from me, consulting or hands on development. As I say I am taking a bite out of technology so you don&#8217;t have to. All the years of experience with corporate tech and now several years out in the wild having to use what I talk about gives me a view on the world that many people don&#8217;t have time to consider, in person, in writing, on the TV, on stage, in the lab. I cover how technology feels and changes your life as much as the more obvious version x with version y tech.</p>
<p>In conferences I am usually the one put there to shake everybody up. So if you need a jolt of inspiration and a view of the future. well thats Feeding Edge and epredator. Cue show reels again <img src="https://s.w.org/images/core/emoji/72x72/1f642.png" alt="🙂" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p></blockquote>
<p><iframe width="560" height="315" src="http://www.youtube.com/embed/Mw_WJgBWoxI" frameborder="0" allowfullscreen></iframe></p>
<p>Well that&#8217;s it from Ian again for now.  It&#8217;s really good to hear him talking in a wider context again, reading about the mix of drawing inspiration from such a wide variety of sources is really refreshing.  It&#8217;s certainly reminded me to go &#8220;heads up&#8221; more often than I generally manage to do, so easy is it to keep too narrow a view on your immediate work tasks.</p>
<p>Thanks Ian, it&#8217;s been a pleasure &#8211; as always!</p>
]]></content:encoded>
			</item>
	</channel>
</rss>
